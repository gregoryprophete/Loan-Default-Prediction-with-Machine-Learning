{"cells":[{"cell_type":"markdown","metadata":{"id":"THAjbfiisES8"},"source":["# Vehicle Loan Prediction Machine Learning Model\n","\n","# Chapter 7 - Random Forest\n","\n","### Recap and Load\n","- As always, let's begin by importing our libraries and loading the data\n","- Notice that we are importing RandomForestClassifier from sklearn.ensemble\n","\n","*Throughout this chapter you may see slightly different results to those on the demo videos. The outputs vary due to the random nature of the random forest algorithm but they should be similar to those in the videos*\n","\n","*Some of the models we will build here are a bit more complex, if you are running into memory related issues try and free up memory by closing down any programs that you do not need to complete the chapter*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-z3Jz_psES_"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, recall_score, roc_curve, auc, precision_score, plot_confusion_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIk3Wl4VsETA"},"outputs":[],"source":["loan_df = pd.read_csv('../data/vehicle_loans_feat.csv', index_col='UNIQUEID')"]},{"cell_type":"markdown","metadata":{"id":"5WDdCs7zsETA"},"source":["Just like we did for Logistic Regression let's convert our categorical variables to the 'category' data type"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sod7X5FJsETB"},"outputs":[],"source":["#convert to category"]},{"cell_type":"markdown","metadata":{"id":"T1jiEt4OsETB"},"source":["Now we can bring the plot_roc_curve and eval_model functions we defined in chapter 6"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"looCdHgesETB"},"outputs":[],"source":["#get plot roc curve"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SJ5ZMydHsETC"},"outputs":[],"source":["#get eval models"]},{"cell_type":"markdown","metadata":{"id":"UecvyOk-sETC"},"source":["\n","## Lesson 1 - Building The Forest\n","\n","In this lesson, we will use the [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) from sklearn to build a Random Forest model for our data\n"]},{"cell_type":"markdown","metadata":{"id":"uw_12zqlsETD"},"source":["### EXERCISE \n","\n","- We seem to be duplicating the code for creating training/test sets and dummy variables \n","- Fill in the function definition below to take in a data frame, create dummy variables and split the data into train/test sets \n","- The return statement has been filled out for you"]},{"cell_type":"markdown","metadata":{"id":"Erk10npcsETE"},"source":["### SOLUTION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTHYjsAasETE"},"outputs":[],"source":["def encode_and_split(loan_df):\n","    #type solution here\n","\n","    return x_train, x_test, y_train, y_test"]},{"cell_type":"markdown","metadata":{"id":"Ro29PqRisETE"},"source":["Now let's test our new function and create a training and test set for RandomForest, this time using the full set of features "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iX7-q9JKsETF"},"outputs":[],"source":["#run encode and split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QlT3XlFNsETF"},"outputs":[],"source":["#get training shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XM3gZGNfsETF"},"outputs":[],"source":["#get testing shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHOBdHZIsETF"},"outputs":[],"source":["#check class distribution"]},{"cell_type":"markdown","metadata":{"id":"7AaVFyE-sETF"},"source":["Ok great, looks like we have a train and test set with the class distribution we want\n","\n","### EXERCISE \n","\n","- Use [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to train and evaluate a Random Forest Model\n","- HINT: The model can be trained using the [fit](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit) function\n","\n","### SOLUTION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zltb5-40sETG"},"outputs":[],"source":["#type solution here"]},{"cell_type":"markdown","metadata":{"id":"e-0Uk_94sETG"},"source":["Let's take a minute to interpret these results \n","\n","### Accuracy \n","\n","- ~78% similar to the simple logistic regression model we built already\n","\n","### Precision \n","\n","- 39% better than simple logistic Regression which had ~33% \n","- More of the instances we classified as defaults actually were defaults \n","- However, most of the instances we classify as defaults are actually not defaults\n","\n","### Recall \n","\n","- Recall has increased dramatically, from 0.03% to 4.5%!\n","- Random Forest picked up a lot more of the actual positive cases\n","- It still missed most of them\n","\n","### F1\n","\n","- The F1 score has also increased dramatically from 0.0006 to ~0.08! \n","- There is a better balance between Precision and Recall for Random Forest\n","- Although this is still generally poor\n","\n","### AUC \n","\n","- The area under the roc curve has increased very slightly\n","\n","### Probability Distributions \n","\n","- Plot shows bad class separation \n","- Majority of cases unlikely to be classified as defaults \n","\n","Generally the random forest is better than Logistic Regression but it is still not doing a good job"]},{"cell_type":"markdown","metadata":{"id":"xI_dKTERsETG"},"source":[]},{"cell_type":"markdown","metadata":{"id":"UeKji9HOsETG"},"source":["## Lesson 2 - Overfitting\n","\n","A model is said to be overfitted if it performs very well on training data but does not generalize well to unseen test data\n","\n","We can look at evaluate our model's performance on the training data to investigate overfitting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CWdUS05RsETH"},"outputs":[],"source":["#eval on training data"]},{"cell_type":"markdown","metadata":{"id":"Pdp-NHd1sETH"},"source":["Wow! Pretty clear evidence that our random forest is overfitting, it has nearly perfect results on the training data and poor results on the test data"]},{"cell_type":"markdown","metadata":{"id":"syQCyWitsETH"},"source":["## Lesson 3 - Hyperparameters "]},{"cell_type":"markdown","metadata":{"id":"RyGAZym7sETH"},"source":["Classification performance of random forest can be heavily influenced by its hyperparameters\n","\n","### Hyperparameter Tuning \n","\n","- The process of selecting optimal hyperparameters \n","- Can be tricky and time-consuming, many automated methods exists for finding the parameters that yield the best classification results \n","- Out of scope of this course but if you are interested look at [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n","\n","### Number of Trees\n","- How big is the forest?\n","- Typically increasing the number of trees increases model performance up to a certain point\n","- Too many trees can increase the computational cost and does little to benefit classification performance\n","- set through the n_estimators property\n","\n","### Maximum Depth\n","- The longest path between a tree root node and its deepest leaf node\n","- exposed through max_depth parameter which defaults to None, meaning the max depth is not limited\n","- limiting the depth of the trees can be used to reduce overfitting\n","\n","\n"," \n","\n"]},{"cell_type":"markdown","metadata":{"id":"FP81xjbQsETI"},"source":["### Number of Trees \n","\n","Let's do some manual exploration of the forest size parameter, remember the default value is 100 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDqwvgXOsETI"},"outputs":[],"source":["# 1 estimator"]},{"cell_type":"markdown","metadata":{"id":"sU__zHIrsETJ"},"source":["- With a forest size of 1, the random forest behaves as a standalone decision tree and is unable to distinguish between the two classes\n","- With AUC of 0.52 it is only marginally better than a random classifier\n","\n","Let's see what happens if we increase the number of trees to 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gWST8o7vsETJ"},"outputs":[],"source":["#10 estimators"]},{"cell_type":"markdown","metadata":{"id":"0JNCWPRfsETJ"},"source":["- We see here that with a forest size of 10 the separation ability of the model increases with an AUC of 0.58\n","- Multiple peaks on the distribution chart suggest that this is not a very stable model\n","\n","How about with the default value of 100 trees?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XrhFtGMGsETJ"},"outputs":[],"source":["#100 estimators\n"]},{"cell_type":"markdown","metadata":{"id":"jVdgkzrTsETJ"},"source":["- With 100 estimators the AUC improved from 0.58 to 0.62\n","- Class distributions appeared more defined and settled\n","\n","What about if we increase to 300?\n","\n","*NB - You might not be able to run all the scenarios due to system capacity. If you receive a \"MemoryError\", it means the model is too expensive to run on your computer. Try reducing n_estimators to 200 or 150.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9dqUrSeOsETK"},"outputs":[],"source":["#300 estimators"]},{"cell_type":"markdown","metadata":{"id":"VC4Lsu5dsETK"},"source":["Very similar performance to the default value of 100! \n","\n","Increasing the size of the forest helps classification performance up to a point\n","\n","However, it also increases the computational cost of training the model"]},{"cell_type":"markdown","metadata":{"id":"T2GrM71GsETK"},"source":["### Maximum Depth\n","\n","We observed earlier that our random forest model is overfitting\n","\n","One way of tackling overfitting in random forest is by limiting the Maximum Depth of the trees. This prevents the classifiers from growing to large picking up noise in the training data\n","\n","The default value of max_depth is None (it is not limited!)\n","\n","Let's do some experiments\n","\n","*NB - You might not be able to run all the scenarios due to system capacity. If you receive a \"MemoryError\", it means the model is too expensive to run on your computer.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_-Gn64hsETK"},"outputs":[],"source":["#max depth 5"]},{"cell_type":"markdown","metadata":{"id":"sinp4vrQsETK"},"source":["We have increased the AUC but the model is failing to identify any loan defaults\n","\n","Let's take a look at how it performs on the training data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qFuh9Jk4sETK"},"outputs":[],"source":["#check the overfitting"]},{"cell_type":"markdown","metadata":{"id":"Fv9n81ebsETK"},"source":["As with the test data, the model is not identifying any defaults.\n","\n","Very similar performance between training and test data tells us we are not overfitting anymore, but the model has very little predictive power\n","\n","Limiting the tree size to 5 has probably oversimplified the model and actually given us an underfit model!\n","\n","Let's try again with a larger max_depth"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wbMhL6i8sETL"},"outputs":[],"source":["#max depth 15"]},{"cell_type":"markdown","metadata":{"id":"PEyRlPyZsETL"},"source":["A few things to note here! \n","\n","We have increased the AUC to ~0.65, this model has the best ability to separate classes that we have seen so far! \n","\n","It is also has a very good precision score of 67%, but we are still identifying very few loan defaults hence the poor recall\n","\n","Let's have a look at the training set performance!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YwvRNPZdsETL"},"outputs":[],"source":["#check the overfitting"]},{"cell_type":"markdown","metadata":{"id":"nlydCEfSsETL"},"source":["Our model does perform better on the training data so it could be a little overfitted. However, it certainly is much less dramatic than before! \n","\n","We have now limited the complexity of the trees in our forest which has reduced overfitting. "]},{"cell_type":"markdown","metadata":{"id":"uwByeQLXsETL"},"source":["### A Note on Hyperparameter Tuning \n","\n","- We have discussed the effects of n_estimators and max_depth in isolation \n","- Random Forest has many more parameters that can be tuned \n","- In reality, parameters are dependent on each other, i.e changing one affects the others\n","- Automated methods to find the right balance exist, look at [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9KucBPSTsETL"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"chapter_7_student.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}
